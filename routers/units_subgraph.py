"""
this file contains the Units subgraph that is used to handle user interactions within the Units chatbot.
Units subgraph is a sequence of nodes that:
1. language: Detects the language of the user input and greets the user if the conversation is just starting.
2. conversation: Processes the conversation by appending the user input to the chat history, generating a response using an LLM, and prompting the user if they want to continue.
3. completion: Checks if enough conversation has occurred to warrant summarization, extracts the conversation history, and generates a summary from it.
4. check_complete: Verifies if a summary has been successfully generated by setting a completion flag.
5. branch: Depending on the existence of a conversation summary, either performs property extraction from the summary or sends a follow-up message to the user.
The subgraph terminates upon reaching the END state and returns the final state.
"""
import asyncio
from typing import Dict, TypedDict
from langgraph.graph import StateGraph, END  # LangGraph’s StateGraph
from langchain.prompts import ChatPromptTemplate, MessagesPlaceholder
from langchain_openai import ChatOpenAI
from langchain_core.output_parsers import StrOutputParser
from core import settings 
from services import PropertyChain
from langchain_core.messages import HumanMessage, AIMessage
from langchain_core.messages import trim_messages
from typing import List, Optional
from format import (
    get_system_prompt_units,
    get_analysis_prompt_check_complete,
    get_summary_prompt,
    get_greeting,
    get_follow_up_message,
)

# Instantiate LLMs and the trimmer.
conv_llm = ChatOpenAI(
    model=settings.MODEL_NAME,
    temperature=0.3,
    openai_api_key=settings.OPENAI_API_KEY
)
checker_llm = ChatOpenAI(
    model=settings.MODEL_NAME,
    temperature=0,
    top_p=0,
    openai_api_key=settings.OPENAI_API_KEY
)
summary_llm = ChatOpenAI(
    model=settings.MODEL_NAME,
    temperature=0.1,
    top_p=0,
    openai_api_key=settings.OPENAI_API_KEY
)

trimmer = trim_messages(strategy="last", max_tokens=16, token_counter=len)

# Define a state type for the UNITS subgraph.
class UnitsChatbotState(TypedDict):
    user_input: str
    extracted_info: Dict | None
    conversation_summary: str | None
    bot_response: str | None
    chat_history: list      # list of messages (HumanMessage, AIMessage, etc.)
    lang: str | None
    should_complete: bool | None


# --- Node 1: Language Detection & Greeting 
# --------------------------------------------------------------------------
async def language_detection_greeting(state: UnitsChatbotState) -> UnitsChatbotState:
    """
    function to detect the language of the user input and greet the user
    """
    messages = state["chat_history"]  
    if not messages:  # If no messages, greet the user.
        state["lang"] = (
            "ar" if any(0x0600 <= ord(c) <= 0x06FF for c in state["user_input"])
            else "en"
        )
        greeting = get_greeting(state["lang"])
        messages.append(AIMessage(content=greeting))
        state["bot_response"] = greeting
    return state

# --- Node 2: Conversational Response 
# --------------------------------------------------------------------------
async def conversational_response(state: UnitsChatbotState) -> UnitsChatbotState:
    """
    this conversation node is responsible for the conversation between the user and the system
    taking the user input and generating a response from the system
    """
    messages = state["chat_history"]
    messages.append(HumanMessage(content=state["user_input"]))
    #trim the messages
    messages_trimmed = trimmer.invoke(messages)  


    if "units_chat_history" not in state:
        state["units_chat_history"] = []
    state["units_chat_history"].append(state["user_input"]) 
    answers_user_list = trimmer.invoke(state["units_chat_history"])
    chat_history_answers_user = extract_chat_history(answers_user_list)
    answers_user = "\n".join([f"{item['role']}: {item['content']}" for item in chat_history_answers_user])

    conversation_chain = (
        ChatPromptTemplate.from_messages([
            MessagesPlaceholder(variable_name="history"),
            ("system", get_system_prompt_units(state['lang'], answers_user)),
            ("human", "{input}"),
        ])
        | conv_llm
        | StrOutputParser()
    )
 
    bot_response = await asyncio.to_thread(
        conversation_chain.invoke,
        {"input": state["user_input"], "history":messages_trimmed}
    )
    messages.append(AIMessage(content=bot_response))
    fixed_message = (
        "هل تريد متابعة البحث، هل المعلومات المقدمة كافية؟"
        if state["lang"] == "ar"
        else "Would you like to proceed with the search, is the provided information sufficient?"
    )
    
    state["bot_response"] = bot_response + "\n\n" + fixed_message
    return state

# --- Node 3: Completion Check & Summary Generation 
# --------------------------------------------------------------------------
def extract_chat_history(messages):
    """
    Extracts the chat history from the list of messages.
    """
    extracted = []
    for msg in messages:
        if isinstance(msg, HumanMessage):
            extracted.append({"role": "user", "content": msg.content})
        elif isinstance(msg, AIMessage):
            extracted.append({"role": "ai", "content": msg.content})
        elif isinstance(msg, dict) and "role" in msg and "content" in msg:
            extracted.append(msg)
    return extracted

# Completion Check & Summary Generation
async def completion_check_and_summary(state: UnitsChatbotState) -> UnitsChatbotState:
    messages = state["chat_history"]

    user_messages = [msg for msg in messages if isinstance(msg, HumanMessage)]
    if len(user_messages) >= 2:
        last_user_message = state["user_input"]
        last_messages = messages[-2:]
        combined = f"last user message: {last_user_message}\nlast Ai message: {' '.join([msg.content for msg in last_messages])}"

        analysis_prompt_text = get_analysis_prompt_check_complete(combined)
        result = await asyncio.to_thread(checker_llm.invoke, analysis_prompt_text)
        if result.content.strip().upper() == "YES":
            #extract the history of the chat that have HumanMessage and AIMessage write the role first (user or ai)and then the content

            chat_history_extracted = extract_chat_history(messages)
            state["extracted_chat_history"] = chat_history_extracted

            history = "\n".join([f"{item['role']}: {item['content']}" for item in chat_history_extracted])
            summary_prompt_temp = get_summary_prompt(state["lang"] or "en") + "\n".join(
                [f"chat history: {history}"]
            ) + "\nSummary:"

            summary_result = await asyncio.to_thread(summary_llm.invoke, summary_prompt_temp)

            state["conversation_summary"] = summary_result.content.strip()
            state["should_complete"] = True
        else:
            state["should_complete"] = False
    return state

# --- Node 4: Check Completion Flag ---
async def check_completion_flag(state: UnitsChatbotState) -> UnitsChatbotState:
    state["should_complete"] = bool(state.get("conversation_summary"))
    return state

# --- Node 5: Property Extraction ---
async def property_extraction(state: UnitsChatbotState) -> UnitsChatbotState:
    if state.get("conversation_summary"):
        chain = PropertyChain()
        try:
            response = await chain.gen_extracted_info(state["conversation_summary"])
            if len(response["data"]["property_specs"]) == 0:
                follow_up = get_follow_up_message(state["lang"] or "en")
                messages = state["chat_history"]
                messages.append(AIMessage(content=follow_up))
                state["bot_response"] = follow_up
                return state
            state["extracted_info"] = response
            state["bot_response"] = f"{response}"
            state["should_complete"] = True
        except Exception as e:
            print(f"Extraction error: {e}")
            state["conversation_summary"] = None
            state["should_complete"] = False
    return state

# ---- Node 6: Final Assembly (Fallback) ----
async def final_assembly(state: UnitsChatbotState) -> UnitsChatbotState:
    if not state.get("extracted_info"):
        follow_up_message = get_follow_up_message(state["lang"] or "en")
        messages = state["chat_history"]
        messages.append(AIMessage(content=follow_up_message))
        state["bot_response"] = follow_up_message
        state["should_complete"] = False
    return state

# ---  Node 7: Branching (Choose the next step) ---
async def branching(state: UnitsChatbotState) -> UnitsChatbotState:
    if state.get("conversation_summary"):
        state = await property_extraction(state)
    else:
        state = await final_assembly(state)
    return state

# ---- Assemble the Sub-Graph ----
def compile_units_chatbot_graph() -> StateGraph:
    graph = StateGraph(UnitsChatbotState)
    graph.add_node("language", language_detection_greeting)
    graph.add_node("conversation", conversational_response)
    graph.add_node("completion", completion_check_and_summary)
    graph.add_node("check_complete", check_completion_flag)
    graph.add_node("branch", branching)
    graph.set_entry_point("language")
    graph.add_edge("language", "conversation")
    graph.add_edge("conversation", "completion")
    graph.add_edge("completion", "check_complete")
    def complete_router(state: UnitsChatbotState):
        if state.get("should_complete"):
            return "branch"
        elif state.get("needs_retry", False):
            return "conversation"
        else:
            return END
    graph.add_conditional_edges("check_complete", complete_router,
                                {"conversation": "conversation", "branch": "branch", END: END})
    graph.add_edge("branch", END)
    compile_graph = graph.compile()
    #display(Image(compile_graph.get_graph().draw_mermaid_png()))
    compile_graph.get_graph().draw_mermaid_png(output_file_path="units_chatbot_subgraph.png")
    return compile_graph

# ---- Units Chatbot Graph ----
class UnitsChatbotGraph:
    def __init__(self):
        self.compiled_graph = compile_units_chatbot_graph()
        #display(Image(self.compiled_graph.get_graph().draw_mermaid_png()))
    
    async def run(self, state: UnitsChatbotState) -> UnitsChatbotState:
        async for output in self.compiled_graph.astream(state):
            final_state = output  # Final state upon reaching END.
        return final_state


# In this method, we expect the parent graph to pass a shared list of messages.
units_chatbot_graph = UnitsChatbotGraph()

# ---- Chatbot Interface ----
async def units_chatbot_interface(user_input: str, user_id: str, history: Optional[List[dict]] = None) -> dict:
    from main_graph import user_memory_store, get_memory_key
    # Define the namespace for units chatbot history
    units_key = get_memory_key(user_id, "units_chat_history")
    
    # Retrieve stored history for this user.
    stored = user_memory_store.get(units_key, units_key)
    chat_history = stored.value if stored else []
    
    # initialize the history list from it.
    if not chat_history and history:
        for m in history:
            if m.get("role") == "user":
                chat_history.append(HumanMessage(content=m.get("content")))
            elif m.get("role") == "ai":
                chat_history.append(AIMessage(content=m.get("content")))
    
    # Build the initial state for the units subgraph
    state: UnitsChatbotState = {
        "user_input": user_input,
        "extracted_info": None,
        "conversation_summary": None,
        "bot_response": None,
        "chat_history": chat_history,  # This is the conversation history loaded from the store.
        "lang": "ar" if any(0x0600 <= ord(c) <= 0x06FF for c in user_input) else "en",
        "should_complete": False,
    }
    
    # Run the units subgraph (your existing UnitsChatbotGraph class)
    final_state = await units_chatbot_graph.run(state)
    
    # After execution, update the stored history with the new state.
    user_memory_store.put(units_key, units_key, final_state.get("chat_history", []))
    
    # Return the bot's response and the updated chat history.
    return {"text": final_state.get("bot_response", ""), "chat_history": final_state.get("chat_history", [])}
